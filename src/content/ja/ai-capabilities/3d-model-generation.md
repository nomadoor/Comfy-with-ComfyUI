---
layout: page.njk
lang: ja
section: ai-capabilities
slug: 3d-model-generation
navId: 3d-model-generation
title: 3Dモデル生成
summary: "3マルチビューからワールドモデルまで"
permalink: /{{ lang }}/{{ section }}/{{ slug }}/
hero:
  image: ''
---

## 3D生成

3D生成は、その名の通りテキストプロンプトや参照画像を元に3Dモデルを作成するタスクです。

text2imageのように、シンプルにノイズからモデルが現れてくれればよいのですが、動画が時間軸方向に1次元多いように、3D生成は空間方向に次元が増えるため、簡単には実現できません。

先に伝えておくと、3D生成はまだ、プロユースに至るほどの性能は得られていません。
しかし、画像からモデルを作り出し、果ては歩き回れる世界を作り出す技術は着々と成長しています。

発展真っ最中であるこの技術の流れを、少し追ってみましょう。

※時系列的にも、技術のつながり的にもかなり適当です。ふんわり眺めてください。

---

## マルチビュー生成

元々NeRFといった画像から3D空間・モデルを作る技術は存在しました。
しかし、NeRFで3Dを構築するには、同じオブジェクトを様々な視点から見た画像が必要になります。

この観点で生まれたのがマルチビュー生成です。

### Zero-1-to-3

> ![](https://gyazo.com/8f4d195ac2daffffc7356a036d4a3c98){gyazo=image}
> [Zero-1-to-3: Zero-shot One Image to 3D Object](https://zero123.cs.columbia.edu/)

拡散モデルをベースにした最初期のマルチビュー生成で、入力画像のカメラの構図を変更した新しい視点の画像を生成します。

当時、3D生成に関係なく便利そうだと思いましたが、要求スペックが高く使えませんでした。  
現在は、指示ベース画像編集で似たようなことが簡単に出来ますね。

### Zero123++

Zero-1-to-3は「入力画像の別角度の画像を1枚作る → 角度を変えて何度も回す」という使い方でしたが、**[Zero123++](https://github.com/SUDO-AI-3D/zero123plus)** は同時に複数視点を生成します。

![](https://gyazo.com/b6b4e05ace668acfd75449b8252b139f){gyazo=image} ![](https://gyazo.com/59359f3b3b6f250358211d2044d207fe){gyazo=image}

元々拡散モデルは、複数枚をバッチ生成（cf. [Batch・動画](/ja/data-utilities/batch-video)）すると、生成した画像同士はある程度一貫性を持つことが知られていました。

3D生成には最初から全方位の複数枚の画像が必要です。  
Zero123++ はその性質を利用して、「一回の生成で、できるだけ一貫したマルチビューをまとめて作る」方向に振ったモデルだといえますね。

---

## 動画生成モデルの登場

少し遅れて、動画生成ができるモデルが登場し始めます。

ここで、マルチビュー生成を「画像編集の一種」として扱うのではなく、

> 対象オブジェクトの周りをぐるっと回りながら撮影した動画
> ＝ とても細かく刻まれたマルチビュー

として扱えばよいのでは？という発想が出てきます。

### Stable Video 3D

[Introducing Stable Video 3D](https://stability.ai/news/introducing-stable-video-3d)

Stable Video Diffusionをベースにした image2model です。

![](https://gyazo.com/49e94de4d1476e100761e2e6be7a2f6e){gyazo=image}

[](/workflows/ai-capabilities/3d-model-generation/SV3D.json)

* 静止画を1枚入力
* そのオブジェクトがぐるっと回転する360度動画を生成
* 動画の各フレームを別視点画像として扱い、そこから3Dを復元

動画生成モデルを3Dモデル生成に応用するという流れ自体は現在も続いています。

現在の動画生成モデルはこのときよりはるかに高性能なので、専用のファインチューニングを施さなくとも、高精細な360度回転動画を生成できますね。

---

## image→3Dモデルを直接ねらうモデル

ここまでは、

* まずマルチビュー（または回転動画）を集める
* それを別の仕組みで3Dに起こす

という「二段構成」が前提でした。

そこから、さらに一歩進んで、

> 入力は画像（またはテキスト）、出力はいきなり3Dモデル

という形を正面からねらうモデルが出てきています。

### Hunyuan3D-2.1

Hunyuan3D-2.1は、画像やテキストから3Dアセットを作るための大規模モデルです。

* まず「形」の部分だけを出すステージ（粗い 3D 形状）
* そのあとで、PBRテクスチャを含む高解像度の見た目を貼るステージ

という二段構成になっています。

### SAM 3D Objects

SAM 3D Objects は、1枚の実写画像から3Dオブジェクトを復元するモデルです。

* 2D側では SAM 系のセグメンテーションを使い、対象物体をしっかり切り出す
* 切り出した領域を手がかりに、隠れている部分も補完しながら 3D 形状とテクスチャを推定する

という流れになっています。


技術的な中身は全く別物ですが、どちらも「image → 3Dモデル」を正面から解こうとしているものたちです。

---

## World モデル

ここまでは、単一オブジェクトを3Dモデル化する話でした。
一方で、写真から世界丸ごと作ろうという試みも進んでいます。

> ここで言う「World モデル」は、世界モデル（物理の予測）というよりも、あくまで **3Dワールド（シーン）を構築するモデル** という意味です。

### 360度パノラマ生成

スタートは360度パノラマ生成です。

Latent Labs のツールや、[HunyuanWorld-1.0](https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0)などが当たります。

* 入力画像をパノラマ球面に貼り付ける
* 映っていない方向を outpainting で補う

というシンプルな発想で、「とりあえず 360 度埋まった見た目」を作ります。

この段階ではまだ3Dとは言えませんが、
ここに深度マップやメッシュ復元を組み合わせて、奥行きのある3D空間を構築しようとしています。

### HunyuanWorld-Mirror

[HunyuanWorld-Mirror](https://github.com/Tencent-Hunyuan/HunyuanWorld-Mirror) になると、より本質的に歩き回れる世界を作ることに近づきます。

> ![](https://gyazo.com/47ddf0aa2f5bc667b43be75d2ed1223c){gyazo=player}

* 画像（または動画）を入力にしてカメラの情報や深度、3D表現（3D Gaussianなど）をまとめて推定するといったコンポーネントで構成されています。
